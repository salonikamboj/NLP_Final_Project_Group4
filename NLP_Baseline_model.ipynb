{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI92ju44jq6a",
        "outputId": "6876eb42-6eb6-481f-b78a-2a470dcbc969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.55.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch scikit-learn pandas accelerate matplotlib seaborn numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"shanegerami/ai-vs-human-text\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQxIw2ZW7Hzi",
        "outputId": "e94c0cf8-ccf5-483e-f307-2ab5a7d8485b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/ai-vs-human-text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(f\"{path}/AI_Human.csv\")\n",
        "\n",
        "# Preview the data\n",
        "print(df.head())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE1FzycQ7LPE",
        "outputId": "fd0d8f4e-4d5f-403e-88e8-9add01ec4435"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text  generated\n",
            "0  Cars. Cars have been around since they became ...        0.0\n",
            "1  Transportation is a large necessity in most co...        0.0\n",
            "2  \"America's love affair with it's vehicles seem...        0.0\n",
            "3  How often do you ride in a car? Do you drive a...        0.0\n",
            "4  Cars are a wonderful thing. They are perhaps o...        0.0\n",
            "(487235, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Split manually into train and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['generated'])\n",
        "\n",
        "\n",
        "train_df_small = train_df.groupby('generated').apply(\n",
        "    lambda x: x.sample(frac=0.03, random_state=42)\n",
        ")\n",
        "\n",
        "test_df_small = test_df.groupby('generated').apply(\n",
        "    lambda x: x.sample(frac=0.03, random_state=42)\n",
        ")\n",
        "\n",
        "# Preview the reduced datasets\n",
        "print(\"Train Shape:\", train_df_small.shape)\n",
        "print(\"Test Shape:\", test_df_small.shape)\n",
        "print(\"\\nTrain Class Distribution:\")\n",
        "print(train_df_small['generated'].value_counts(normalize=True))\n",
        "print(\"\\nTest Class Distribution:\")\n",
        "print(test_df_small['generated'].value_counts(normalize=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srURmyDm7NbV",
        "outputId": "15110dce-99f4-4eca-91ef-2e73fcf34617"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Shape: (11693, 2)\n",
            "Test Shape: (2924, 2)\n",
            "\n",
            "Train Class Distribution:\n",
            "generated\n",
            "0.0    0.62764\n",
            "1.0    0.37236\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test Class Distribution:\n",
            "generated\n",
            "0.0    0.627565\n",
            "1.0    0.372435\n",
            "Name: proportion, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2309301939.py:10: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  train_df_small = train_df.groupby('generated').apply(\n",
            "/tmp/ipython-input-2309301939.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  test_df_small = test_df.groupby('generated').apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A training set of 11,693 samples and a testing set of 2,924 samples have been created from the downsampled dataset.  With around 63% of the text in both datasets being human-generated (class 0.0) and 37% being AI-generated (class 1.0), the class distribution is still unbalanced."
      ],
      "metadata": {
        "id": "b5u5tkPPZZ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution before undersampling\n",
        "print(\"Train class distribution before undersampling:\")\n",
        "print(train_df_small['generated'].value_counts())\n",
        "\n",
        "# Separate human and ai generated classes\n",
        "human = train_df_small[train_df_small['generated'] == 0.0]\n",
        "ai = train_df_small[train_df_small['generated'] == 1.0]\n",
        "\n",
        "# Separate human and ai generated for test data\n",
        "human_test = test_df_small[test_df_small['generated'] == 0.0]\n",
        "ai_test = test_df_small[test_df_small['generated'] == 1.0]\n",
        "\n",
        "# Undersample human class in train data to match ai class size\n",
        "human_undersampled = human.sample(n=len(ai), random_state=42)\n",
        "\n",
        "# Combine ai class with downsampled human class for train data\n",
        "train_df_balanced = pd.concat([human_undersampled, ai])\n",
        "\n",
        "# Shuffle the balanced train dataset\n",
        "train_df_balanced = train_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Undersample human class in test data to match ai class size\n",
        "human_undersampled_test = human_test.sample(n=len(ai_test), random_state=42)\n",
        "\n",
        "# Combine ai class with downsampled human class for test data\n",
        "test_df_balanced = pd.concat([human_undersampled_test, ai_test])\n",
        "\n",
        "# Shuffle the balanced test dataset\n",
        "test_df_balanced = test_df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Check class distribution after undersampling for train data\n",
        "print(\"\\nTrain class distribution after undersampling:\")\n",
        "print(train_df_balanced['generated'].value_counts(normalize=True))\n",
        "\n",
        "# Check class distribution after undersampling for test data\n",
        "print(\"\\nTest class distribution after undersampling:\")\n",
        "print(test_df_balanced['generated'].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io49XL3i7RxH",
        "outputId": "064c243c-b2f6-4a13-daeb-02a2c1b64cbd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution before undersampling:\n",
            "generated\n",
            "0.0    7339\n",
            "1.0    4354\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train class distribution after undersampling:\n",
            "generated\n",
            "0.0    0.5\n",
            "1.0    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Test class distribution after undersampling:\n",
            "generated\n",
            "0.0    0.5\n",
            "1.0    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = train_df_balanced.reset_index(drop=True)\n",
        "\n",
        "# Character length summary by class\n",
        "char_length_stats = df.groupby('generated')['text'].apply(lambda x: x.str.len().describe())\n",
        "print(\"Character Length Summary by Class:\\n\", char_length_stats)\n",
        "\n",
        "# Word count summary by class\n",
        "word_count_stats = df.groupby('generated')['text'].apply(lambda x: x.str.split().apply(len).describe())\n",
        "print(\"\\nWord Count Summary by Class:\\n\", word_count_stats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnTKepHz7qbP",
        "outputId": "1d2424b6-0270-42a1-a3b7-4629134cd282"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Length Summary by Class:\n",
            " generated       \n",
            "0.0        count    4354.000000\n",
            "           mean     2336.546624\n",
            "           std      1052.047430\n",
            "           min       269.000000\n",
            "           25%      1541.000000\n",
            "           50%      2140.000000\n",
            "           75%      2891.000000\n",
            "           max      9457.000000\n",
            "1.0        count    4354.000000\n",
            "           mean     2106.435921\n",
            "           std       773.818839\n",
            "           min       272.000000\n",
            "           25%      1632.000000\n",
            "           50%      2030.000000\n",
            "           75%      2457.000000\n",
            "           max      7075.000000\n",
            "Name: text, dtype: float64\n",
            "\n",
            "Word Count Summary by Class:\n",
            " generated       \n",
            "0.0        count    4354.000000\n",
            "           mean      418.721635\n",
            "           std       181.447852\n",
            "           min        61.000000\n",
            "           25%       280.250000\n",
            "           50%       389.000000\n",
            "           75%       515.000000\n",
            "           max      1366.000000\n",
            "1.0        count    4354.000000\n",
            "           mean      342.249655\n",
            "           std       115.780798\n",
            "           min        44.000000\n",
            "           25%       271.000000\n",
            "           50%       336.000000\n",
            "           75%       402.000000\n",
            "           max       920.000000\n",
            "Name: text, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The balanced dataset clearly distinguishes between texts created by AI and those authored by humans, according to the exploratory data analysis.  With an average of 418.7 words and 2336.5 characters, human-written texts (Class 0.0) are often lengthier.  AI-generated texts (Class 1.0), on the other hand, have a mean character count of 2106.4 and a mean word count of 342.2, making them typically shorter and less diversified."
      ],
      "metadata": {
        "id": "6BAR1dHssaO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Set max length for tokens per sample according to EDA 75% of the dataset has text length of 500\n",
        "max_text_length = 500\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Lowercase, lemmatize, remove non-alphabetic and stopwords\n",
        "    processed = [\n",
        "        lemmatizer.lemmatize(word.lower())\n",
        "        for word in tokens\n",
        "        if word.isalpha() and word.lower() not in stop_words\n",
        "    ]\n",
        "    return processed[:max_text_length]\n",
        "\n",
        "# Apply preprocessing to balanced train and test data\n",
        "train_df_balanced['tokens'] = train_df_balanced['text'].apply(preprocess_text)\n",
        "test_df_balanced['tokens'] = test_df_balanced['text'].apply(preprocess_text)\n",
        "\n",
        "# Flatten all tokens from balanced train data to build vocabulary\n",
        "all_tokens = [token for tokens_list in train_df_balanced['tokens'] for token in tokens_list]\n",
        "\n",
        "# Count word frequency\n",
        "word_freq = Counter(all_tokens)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(word_freq)\n",
        "print(f\"Vocabulary size after preprocessing: {vocab_size}\")\n",
        "\n",
        "# Top 10 most common words\n",
        "print(\"\\nTop 10 most common words:\")\n",
        "for word, count in word_freq.most_common(10):\n",
        "    print(f\"{word}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10H7LW28783v",
        "outputId": "0ef6970c-6d78-402e-d6a9-a6d0629516b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size after preprocessing: 28668\n",
            "\n",
            "Top 10 most common words:\n",
            "student: 26200\n",
            "car: 19834\n",
            "people: 19081\n",
            "would: 16175\n",
            "school: 12499\n",
            "also: 11533\n",
            "help: 11432\n",
            "like: 10893\n",
            "time: 10494\n",
            "electoral: 10030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initially, a function to remove stopwords, lowercase, lemmatize, and tokenize text is defined.  The training and testing datasets are then subjected to this function.  The final size of the vocabulary, which is constructed from the processed training data, is presented along with the ten most often used terms.  After preprocessing, there are 49,697 words in the vocabulary.  Among the most commonly used terms are \"car,\" \"people,\" \"student,\" and \"would.\"  Words like \"student,\" \"school,\" and \"electoral\" indicate that the dataset probably includes content that is academic or essay-style."
      ],
      "metadata": {
        "id": "caEcz5HGsqdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df_balanced['generated'].value_counts())\n"
      ],
      "metadata": {
        "id": "ZDIVKUaq9zo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f1a0c14-2131-4aa7-b355-b38f0f2998f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generated\n",
            "0.0    4354\n",
            "1.0    4354\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "# max_features=5000 limits vocab to 5000 most frequent terms (including unigrams and bigrams)\n",
        "\n",
        "# Fit on train tokens joined as strings\n",
        "X_train = vectorizer.fit_transform(train_df_balanced['tokens'].apply(lambda tokens: ' '.join(tokens)))\n",
        "\n",
        "# Transform test tokens joined as strings with the same vectorizer\n",
        "X_test = vectorizer.transform(test_df_balanced['tokens'].apply(lambda tokens: ' '.join(tokens)))\n"
      ],
      "metadata": {
        "id": "zt9u70T787-W"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training data shape:\", X_train.shape)\n",
        "print(\"First few feature names:\", vectorizer.get_feature_names_out()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXEtrvgvF_ya",
        "outputId": "d3d58cbd-19d5-48d3-db93-9ef5aa6ba668"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (8708, 5000)\n",
            "First few feature names: ['aad' 'ability' 'able' 'able attend' 'able drive' 'able get' 'able go'\n",
            " 'able learn' 'able make' 'able see']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize Logistic Regression classifier\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, train_df_balanced['generated'])\n",
        "\n",
        "# Predict on test set\n",
        "test_preds = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "XrHsMQ0jGEnE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
        "\n",
        "# Calculate accuracy\n",
        "acc = accuracy_score(test_df_balanced['generated'], test_preds)\n",
        "print(f\"Logistic Regression Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Display precision, recall, F1 for each class and overall\n",
        "print(\"Classification Report:\\\\n\", classification_report(test_df_balanced['generated'], test_preds, target_names=['0','1']))\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(test_df_balanced['generated'], test_preds)\n",
        "print(\"Confusion Matrix:\\\\n\", cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44MymTkPGKG8",
        "outputId": "3f17fad3-2003-4514-84cb-5070d56003cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9766\n",
            "Classification Report:\\n               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      1089\n",
            "           1       0.98      0.98      0.98      1089\n",
            "\n",
            "    accuracy                           0.98      2178\n",
            "   macro avg       0.98      0.98      0.98      2178\n",
            "weighted avg       0.98      0.98      0.98      2178\n",
            "\n",
            "Confusion Matrix:\\n [[1062   27]\n",
            " [  24 1065]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With an overall accuracy of 97.66%, the model performs admirably.  For both human (class 0) and AI (class 1) texts, the classification report shows strong and balanced accuracy, recall, and F1-score values of 0.98.  The model accurately detected 1062 human texts and 1065 AI-generated texts, with a low amount of misclassifications (27 false positives and 24 false negatives), according to the confusion matrix, which provides more specific information about this performance."
      ],
      "metadata": {
        "id": "oXvOovQrs9rR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=\"In the ever-evolving landscape of technology, artificial intelligence continues to redefine the boundaries of possibility. From healthcare to finance, AI-driven innovations are enhancing decision-making processes, automating complex tasks, and transforming user experiences. As algorithms become more sophisticated, ethical considerations and transparency remain crucial to ensure equitable progress. The future promises even greater integration of AI in our daily lives, reshaping how we interact with the world around us.\"\n",
        "# Preprocess and transform\n",
        "tokens = preprocess_text(sentence)\n",
        "tfidf_vector = vectorizer.transform([' '.join(tokens)])\n",
        "\n",
        "# Predict using the trained classifier\n",
        "predicted_label = clf.predict(tfidf_vector)[0]\n",
        "predicted_prob = clf.predict_proba(tfidf_vector)[0]\n",
        "\n",
        "# Output\n",
        "print(\"Predicted label:\", predicted_label)\n",
        "print(\"Prediction confidence (class probabilities):\", predicted_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sga1YdedGgfv",
        "outputId": "a50d4df3-a236-40b2-de07-b2ba7e0ca126"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: 1.0\n",
            "Prediction confidence (class probabilities): [0.03342533 0.96657467]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence=\"\"\"\n",
        "Dear senator, Retain the Electoral College. The Electoral College consists of 538 electors and a majority of 270 electors is is required to elect the President. Each state has hisher own electors which are chosen by the candidate political party. You should keep the Electoral College because you have certainty of outcome, and the President is everyones not just yours.\n",
        "\n",
        "The first reason why you should stay with the Electoral College is because you are certain that the outcome will be in favor of one of the candidates. A tie in the nationwide electoral vote may happen but it is very unlikely that it will even though that 538number of electors in the Electoral College is a even numberS.3.For example in 2012's election, Obama received 61.7 percent of the electoral votes compared to 51.3 percent of the popular cast for him and rodney because all states award electoral votes on a winnertakeall basis even a slight plurality in a state creates a landslide electoralvote victory in that stateS.3. However,because of the winnertakeall system in each state,candidates dont spend time in staes they know they have no chance of winning, they only focus on the close,tight races in the \"swing\"statesS.2. But, the winning candidates share of the Electoral College invariably exceeds his share of the popular vote.\n",
        "\n",
        "The second reason you should keep the Electoral College is because the president is everyone's. The Electoral College requires a presidential candidate to have transregional appeal. No region has enough electoral votes to elect a president by themselves. So for example,a solid regional favorite,such as rodney was in the South,has no incentive to campaign heavily in those states for he gains no electoral votes by increasing his plurality in states he knows for sure that he will winS.3.A president with only his regional apppeal is very unlikely to be a successful president. The residents of the other regions may feel like there votes dont count or that he really isnt there president.\n",
        "\n",
        "In conclusion, you should stay with the Electoral College simply because you most likely not going to have a tie and because the president is everyone's.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "tokens = preprocess_text(sentence)\n",
        "cleaned_text = ' '.join(tokens)\n",
        "\n",
        "tfidf_vector = vectorizer.transform([cleaned_text])\n",
        "\n",
        "\n",
        "predicted_label = clf.predict(tfidf_vector)[0]\n",
        "predicted_prob = clf.predict_proba(tfidf_vector)[0]\n",
        "\n",
        "\n",
        "print(\"Predicted label:\", predicted_label)\n",
        "print(\"Prediction confidence (class probabilities):\", predicted_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPgY96ecGiDw",
        "outputId": "4212f7d2-d018-444a-edc5-c2a2dac1e647"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: 0.0\n",
            "Prediction confidence (class probabilities): [0.82220408 0.17779592]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence=\"\"\"\n",
        "Dear Senator,\n",
        "\n",
        "Retain the Electoral College. The Electoral College consists of 538 electors, and a majority of 270 electors is required to elect the President. Each state has its own electors, which are chosen by the candidate’s political party. You should keep the Electoral College because it provides certainty of outcome, and the President represents everyone, not just one group.\n",
        "\n",
        "The first reason why you should stay with the Electoral College is because you are certain that the outcome will be in favor of one of the candidates. A tie in the nationwide electoral vote may happen, but it is very unlikely, even though the 538 electors in the Electoral College is an even number. For example, in the 2012 election, Obama received 61.7 percent of the electoral votes compared to 51.3 percent of the popular vote cast for him. This is because all states award electoral votes on a winner-take-all basis — even a slight plurality in a state creates a landslide electoral vote victory in that state. However, because of the winner-take-all system in each state, candidates don’t spend time in states they know they have no chance of winning; they only focus on the close, tight races in the “swing” states. But the winning candidate’s share of the Electoral College invariably exceeds his share of the popular vote.\n",
        "\n",
        "The second reason you should keep the Electoral College is because the President is everyone’s President. The Electoral College requires a presidential candidate to have transregional appeal. No region has enough electoral votes to elect a president by itself. For example, a solid regional favorite, such as Rodney was in the South, has no incentive to campaign heavily in those states, for he gains no additional electoral votes by increasing his plurality in states he knows for sure he will win. A president with only regional appeal is very unlikely to be a successful president. The residents of other regions may feel like their votes don’t count or that he really isn’t their president.\n",
        "\n",
        "In conclusion, you should stay with the Electoral College simply because it is very unlikely that there will be a tie, and because the President is everyone’s.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "tokens = preprocess_text(sentence)\n",
        "cleaned_text = ' '.join(tokens)\n",
        "\n",
        "\n",
        "tfidf_vector = vectorizer.transform([cleaned_text])\n",
        "\n",
        "predicted_label = clf.predict(tfidf_vector)[0]\n",
        "predicted_prob = clf.predict_proba(tfidf_vector)[0]\n",
        "\n",
        "\n",
        "print(\"Predicted label:\", predicted_label)\n",
        "print(\"Prediction confidence (class probabilities):\", predicted_prob)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRfDTQHuf62F",
        "outputId": "b174ef6d-f5c4-46d5-f170-bee82a1dac68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted label: 0.0\n",
            "Prediction confidence (class probabilities): [0.79946925 0.20053075]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model overfits, it has learnt dataset so well, even when a text with typos and grammatical error is entered it shows AI generated."
      ],
      "metadata": {
        "id": "89niH-OKt3PV"
      }
    }
  ]
}